{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oridnary Least Square and Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pymc3 as pm\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = pm.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymc3.model.Model object at 0x1c163d1400>\n"
     ]
    }
   ],
   "source": [
    "print(basic_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a new Model object which is a container for the model random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a context manager, with our basic_model as the context, that includes all statements until the indented block ends. This means all PyMC3 objects introduced in the indented code block below the with statement are added to the model behind the scenes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When the sample function with specifications is added to the model then it samples the posterionr distribution provided by the likelihood and prior.\n",
    " \n",
    "Additionally, the function specifies the type of jumps it will take across the distribution. The default is the NUTS sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sample in module pymc3.sampling:\n",
      "\n",
      "sample(draws=500, step=None, init='auto', n_init=200000, start=None, trace=None, chain_idx=0, chains=None, cores=None, tune=500, nuts_kwargs=None, step_kwargs=None, progressbar=True, model=None, random_seed=None, live_plot=False, discard_tuned_samples=True, live_plot_kwargs=None, compute_convergence_checks=True, use_mmap=False, **kwargs)\n",
      "    Draw samples from the posterior using the given step methods.\n",
      "    \n",
      "    Multiple step methods are supported via compound step methods.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    draws : int\n",
      "        The number of samples to draw. Defaults to 500. The number of tuned\n",
      "        samples are discarded by default. See discard_tuned_samples.\n",
      "    step : function or iterable of functions\n",
      "        A step function or collection of functions. If there are variables\n",
      "        without a step methods, step methods for those variables will\n",
      "        be assigned automatically.\n",
      "    init : str\n",
      "        Initialization method to use for auto-assigned NUTS samplers.\n",
      "    \n",
      "        * auto : Choose a default initialization method automatically.\n",
      "          Currently, this is `'jitter+adapt_diag'`, but this can change in\n",
      "          the future. If you depend on the exact behaviour, choose an\n",
      "          initialization method explicitly.\n",
      "        * adapt_diag : Start with a identity mass matrix and then adapt\n",
      "          a diagonal based on the variance of the tuning samples. All\n",
      "          chains use the test value (usually the prior mean) as starting\n",
      "          point.\n",
      "        * jitter+adapt_diag : Same as `adapt_diag`, but add uniform jitter\n",
      "          in [-1, 1] to the starting point in each chain.\n",
      "        * advi+adapt_diag : Run ADVI and then adapt the resulting diagonal\n",
      "          mass matrix based on the sample variance of the tuning samples.\n",
      "        * advi+adapt_diag_grad : Run ADVI and then adapt the resulting\n",
      "          diagonal mass matrix based on the variance of the gradients\n",
      "          during tuning. This is **experimental** and might be removed\n",
      "          in a future release.\n",
      "        * advi : Run ADVI to estimate posterior mean and diagonal mass\n",
      "          matrix.\n",
      "        * advi_map: Initialize ADVI with MAP and use MAP as starting point.\n",
      "        * map : Use the MAP as starting point. This is discouraged.\n",
      "        * nuts : Run NUTS and estimate posterior mean and mass matrix from\n",
      "          the trace.\n",
      "    n_init : int\n",
      "        Number of iterations of initializer\n",
      "        If 'ADVI', number of iterations, if 'nuts', number of draws.\n",
      "    start : dict, or array of dict\n",
      "        Starting point in parameter space (or partial point)\n",
      "        Defaults to trace.point(-1)) if there is a trace provided and\n",
      "        model.test_point if not (defaults to empty dict). Initialization\n",
      "        methods for NUTS (see `init` keyword) can overwrite the default.\n",
      "    trace : backend, list, or MultiTrace\n",
      "        This should be a backend instance, a list of variables to track,\n",
      "        or a MultiTrace object with past values. If a MultiTrace object\n",
      "        is given, it must contain samples for the chain number `chain`.\n",
      "        If None or a list of variables, the NDArray backend is used.\n",
      "        Passing either \"text\" or \"sqlite\" is taken as a shortcut to set\n",
      "        up the corresponding backend (with \"mcmc\" used as the base\n",
      "        name).\n",
      "    chain_idx : int\n",
      "        Chain number used to store sample in backend. If `chains` is\n",
      "        greater than one, chain numbers will start here.\n",
      "    chains : int\n",
      "        The number of chains to sample. Running independent chains\n",
      "        is important for some convergence statistics and can also\n",
      "        reveal multiple modes in the posterior. If `None`, then set to\n",
      "        either `chains` or 2, whichever is larger.\n",
      "    cores : int\n",
      "        The number of chains to run in parallel. If `None`, set to the\n",
      "        number of CPUs in the system, but at most 4. Keep in mind that\n",
      "        some chains might themselves be multithreaded via openmp or\n",
      "        BLAS. In those cases it might be faster to set this to one.\n",
      "    tune : int\n",
      "        Number of iterations to tune, if applicable (defaults to 500).\n",
      "        Samplers adjust the step sizes, scalings or similar during\n",
      "        tuning. Tuning samples will be drawn in addition to the number\n",
      "        specified in the `draws` argument, and will be discarded\n",
      "        unless `discard_tuned_samples` is set to False.\n",
      "    nuts_kwargs : dict\n",
      "        Options for the NUTS sampler. See the docstring of NUTS\n",
      "        for a complete list of options. Common options are\n",
      "    \n",
      "        * target_accept: float in [0, 1]. The step size is tuned such\n",
      "          that we approximate this acceptance rate. Higher values like 0.9\n",
      "          or 0.95 often work better for problematic posteriors.\n",
      "        * max_treedepth: The maximum depth of the trajectory tree.\n",
      "        * step_scale: float, default 0.25\n",
      "          The initial guess for the step size scaled down by `1/n**(1/4)`.\n",
      "    \n",
      "        If you want to pass options to other step methods, please use\n",
      "        `step_kwargs`.\n",
      "    step_kwargs : dict\n",
      "        Options for step methods. Keys are the lower case names of\n",
      "        the step method, values are dicts of keyword arguments.\n",
      "        You can find a full list of arguments in the docstring of\n",
      "        the step methods. If you want to pass arguments only to nuts,\n",
      "        you can use `nuts_kwargs`.\n",
      "    progressbar : bool\n",
      "        Whether or not to display a progress bar in the command line. The\n",
      "        bar shows the percentage of completion, the sampling speed in\n",
      "        samples per second (SPS), and the estimated remaining time until\n",
      "        completion (\"expected time of arrival\"; ETA).\n",
      "    model : Model (optional if in `with` context)\n",
      "    random_seed : int or list of ints\n",
      "        A list is accepted if `cores` is greater than one.\n",
      "    live_plot : bool\n",
      "        Flag for live plotting the trace while sampling\n",
      "    live_plot_kwargs : dict\n",
      "        Options for traceplot. Example: live_plot_kwargs={'varnames': ['x']}\n",
      "    discard_tuned_samples : bool\n",
      "        Whether to discard posterior samples of the tune interval.\n",
      "    compute_convergence_checks : bool, default=True\n",
      "        Whether to compute sampler statistics like gelman-rubin and\n",
      "        effective_n.\n",
      "    use_mmap : bool, default=False\n",
      "        Whether to use joblib's memory mapping to share numpy arrays when\n",
      "        sampling across multiple cores.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    trace : pymc3.backends.base.MultiTrace\n",
      "        A `MultiTrace` object that contains the samples.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. code:: ipython\n",
      "    \n",
      "        >>> import pymc3 as pm\n",
      "        ... n = 100\n",
      "        ... h = 61\n",
      "        ... alpha = 2\n",
      "        ... beta = 2\n",
      "    \n",
      "    .. code:: ipython\n",
      "    \n",
      "        >>> with pm.Model() as model: # context management\n",
      "        ...     p = pm.Beta('p', alpha=alpha, beta=beta)\n",
      "        ...     y = pm.Binomial('y', n=n, p=p, observed=h)\n",
      "        ...     trace = pm.sample(2000, tune=1000, cores=4)\n",
      "        >>> pm.summary(trace)\n",
      "               mean        sd  mc_error   hpd_2.5  hpd_97.5\n",
      "        p  0.604625  0.047086   0.00078  0.510498  0.694774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pm.sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"yield_forecast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df,test_size):\n",
    "   \n",
    "    # Targets are final grade of student\n",
    "    labels = df['y_pred']\n",
    "    # Drop the school and the grades from features\n",
    "    df = df.drop(columns=['asd_desc','state','y_pred'],axis = 1)\n",
    "    \n",
    "    # One-Hot Encoding of Categorical Variables\n",
    "    #df = pd.get_dummies(df)\n",
    "    \n",
    "    #df['y'] = list(labels)\n",
    "    #most_correlated = df.corr().abs()['y'].sort_values(ascending=False)\n",
    "    #print(most_correlated)\n",
    "    \n",
    "    # Keep correlations greater than 0.2 in absolute value\n",
    "    #most_correlated = most_correlated[most_correlated >= 0.2][1:]\n",
    "    \n",
    "    #df = df.ix[:, most_correlated.index]\n",
    "    #df = df.drop(columns = 'y')\n",
    "    \n",
    "    # Split into training/testing sets with 25% split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, labels, \n",
    "                                                        test_size = test_size,\n",
    "                                                       random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, X_test, y_train, y_test):\n",
    "    # Names of models\n",
    "    model_name = ['OLS']\n",
    "    \n",
    "    # Instantiate the models\n",
    "    model1 = sm.OLS(y_train,X_train)\n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns=['rmse'], index = model_name)\n",
    "    \n",
    "   \n",
    "    fitted = model1.fit()\n",
    "    #print(X_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #print(X_test.shape)\n",
    "    print(fitted.summary())\n",
    "    predictions = fitted.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #mape = np.median(np.divide(abs(predictions - y_test), y_test, where= y_test!=0))\n",
    "        \n",
    "    # Insert results into the dataframe\n",
    "    results.loc[model_name, :] = [rmse]\n",
    "    \n",
    "    # Median Value Baseline Metrics\n",
    "    baseline_mse = np.mean((np.median(y_train_yield) - y_test) ** 2)\n",
    "    baseline_rmse = np.sqrt(baseline_mse)\n",
    "    #baseline_mape = np.median(np.divide(abs(np.mean(y_train) - y_test), y_test, where= y_test!=0))\n",
    "    #print(model1.coef_)\n",
    "    #observation = pd.DataFrame({'cyield':32.8,'irig_flag':0,'dewPoint':38.41,'humidity':0.56,'temp_delta':23.567,'precipIntensity':0.001869},index=0)\n",
    "    #print(model1.predict(observation))\n",
    "    results.loc['Baseline', :] = [baseline_rmse]\n",
    "    \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply format data function\n",
    "X_train_yield, X_test_yield, y_train_yield, y_test_yield = format_data(df,0.50)\n",
    "#X_train_yield.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_pred   R-squared:                       0.994\n",
      "Model:                            OLS   Adj. R-squared:                  0.994\n",
      "Method:                 Least Squares   F-statistic:                     1304.\n",
      "Date:                Tue, 15 May 2018   Prob (F-statistic):           1.88e-94\n",
      "Time:                        06:41:51   Log-Likelihood:                -333.70\n",
      "No. Observations:                 101   AIC:                             691.4\n",
      "Df Residuals:                      89   BIC:                             722.8\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "cyield                     0.5050      0.049     10.392      0.000       0.408       0.602\n",
      "irig_flag                 22.3755      2.876      7.781      0.000      16.661      28.089\n",
      "days_under0             8.373e-11   2.19e-10      0.382      0.703   -3.52e-10    5.19e-10\n",
      "dewPoint                  -3.6976      1.022     -3.619      0.000      -5.728      -1.667\n",
      "precipAccumulation         0.0712      0.089      0.800      0.426      -0.106       0.248\n",
      "precip                    -0.0992      0.053     -1.881      0.063      -0.204       0.006\n",
      "days_under_n10         -1.639e-11   4.42e-11     -0.371      0.711   -1.04e-10    7.14e-11\n",
      "days_over42                0.2804      0.131      2.145      0.035       0.021       0.540\n",
      "days_over32               -0.2371      0.132     -1.798      0.076      -0.499       0.025\n",
      "humidity                 119.1837     42.046      2.835      0.006      35.640     202.728\n",
      "temp_delta                 0.3944      0.349      1.130      0.262      -0.299       1.088\n",
      "temperatureMin             3.3009      1.183      2.791      0.006       0.951       5.651\n",
      "apparentTemperatureMin    -1.1782      1.266     -0.931      0.355      -3.694       1.337\n",
      "precipIntensity         -293.3164    778.785     -0.377      0.707   -1840.745    1254.112\n",
      "==============================================================================\n",
      "Omnibus:                        0.443   Durbin-Watson:                   2.213\n",
      "Prob(Omnibus):                  0.801   Jarque-Bera (JB):                0.549\n",
      "Skew:                           0.146   Prob(JB):                        0.760\n",
      "Kurtosis:                       2.789   Cond. No.                     2.93e+18\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.29e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# apply evaluate function\n",
    "yield_results = evaluate(X_train_yield, X_test_yield, y_train_yield, y_test_yield)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>7.257577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>33.119432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               rmse\n",
       "OLS        7.257577\n",
       "Baseline  33.119432"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yield_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAHMCAYAAAAu4SoNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHPdJREFUeJzt3X20XHV97/H3hyQQ5MEHCBSJkuClFhQMGBBrHyjIg9ar1qtW7BW0Vui1Vrj03vpQ1HrFqmupePukFx9RqcUqVm61VcqyWllVb6IRwYBoDBKIJESoaEFJ+N4/ZkfHw0lOgmfP5Dfn/VrrrMzsvc+Z7wwk77P37JlJVSFJktq027gHkCRJ958hlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZcmkVJliSpJPN3YNvnJ/n8KOYSJPnVJNePew5pthlyzVlJ1ib5cZL9pyxf1cV4yXgm+5lfCH7Qfa1N8vIp2+zQ/EkWJ/loktuS/HuSryV5/jZuZ+vXb29ntlOTfC7JnUk2JvlskqfO+oMwy6rqX6vqkeOeQ5pthlxz3beB07deSXIksOf4xrmPB1XV3sAzgVclOXnK+h2Z/wPATcAhwH7AGcCt093O0Nel0w2T5JnA3wHvBxYDBwKvBv7z/blzo7IjR0ikVhlyzXUfYBC2rc5kEKmfSPLAJO/v9j5vTHJ+kt26dfOSvLnb210D/OY03/vuJOuT3JzkgiTzdnbIqloBXAss29n5gWOB91XVD6tqc1V9par+cWdnSBLgrcDrqupdVfXvVXVvVX22ql7UbbNb9/jcmGRD97g9sFu3de//BUluSnJ7kt9PcmySq5PckeQvh27v+UmuSvIX3ZGE65KcNLT+BUlWd0cG1iQ5e2jdCUnWJXlZku8C7926bGibl3X/Te5Mcv3Wn51kjyRvS3JL9/W2JHtM+bl/1N2/9UlesLOPpTSbDLnmui8A+yY5vAvsbwMfnLLNXwAPBA4Ffp1BOLf+4/0i4CnA0cByBnvOwy4GNgP/qdvmFOD3dnbIJMcDjwa+eT/m/wLwV0mek+ThO3vbQx4JPAz4yHa2eX739RsMHq+9gb+css3jgMO6Wd8G/AnwROBRwLOT/PqUbdcA+wOvAS5L8pBu3QYGj/2+DP57XJjkmKHv/QXgIQyORJw1PECSRwIvAY6tqn2AU4G13eo/AY5n8EvTY4DjgPOn/NwHAgcDL2Tw2D54O4+J1CtDLv10r/Zk4Drg5q0rhuL4iqq6s6rWAm8Bntdt8mzgbVV1U1V9D3jD0PceCDwJOLfbG94AXAg8Zydmuy3JXcC/AX8N/P3OzN95FvCvwKuAb3fPoR87ze3cMfR1+DS3s1/35/rtzPs7wFurak1V/QB4BfCcKYe2X1dVd1fVp4EfAh+qqg1VdXM359FD225g8Pje0x3uv57uqEdVfaKqvlUDnwU+Dfzq0PfeC7ymqn5UVXdNmXMLsAdwRJIFVbW2qr41dB/+VzfTRuC1/PS/N8A93fp7quqTwA8Y/JIjjYXPG0mDEH4OWMp9D0vvD+wO3Di07EYGe2MAD2Xw/PPwuq0OARYA6wdHpYHBL8/D289kf6CAcxk8F74A+PFOzE9V3Q68HHh5d2Lcm4G/T7J4+HaqavMMs2zq/jyIwXPz03ko932s5jN4Ln2r4efn75rm+t5D12+un/1kpxu72yDJkxjspf8ig8f1AcDXhrbdWFV3TzdkVX0zybnAnwKPSvIp4LyqumUb9+GhQ9c3TXms/mPKzNJIuUeuOa+qbmQQpicDl01ZfRuDPbBDhpY9nJ/u9a5ncLh5eN1WNwE/YhDJB3Vf+1bVo3Zyvi1V9RbgbuDFOzn/1G1vYxDyhzI47Lwzrmdwn/7Ldra5hfs+Vpu578l1O+rgDP0W1P28W7rnrD/K4L4cWFUPAj4JDG+73Y92rKq/qapf6eYt4E3buQ+33M/5pd4ZcmnghcCJVfXD4YVVtQX4MPD6JPskOQQ4j58+D/1h4KUZvMTrwQz2fLd+73oGh3vfkmTf7kSwR0x5DnhnvBH44yQLd3R+gCRvSvLoJPOT7AP8N+CbVbXpPj9lO7o94/MYnD3/gqH79CtJLuo2+xDw35MsTbI38GfApTuwt78tBzB4fBckeRZwOINg787g0PhGYHO3d37Kjv7QJI9McmL3C8HdDI4EbBm6D+cnWdQdwXg19z3vQNplGHIJ6J5rXbGN1X/I4LncNcDngb8B3tOteyfwKeCrwJe57x7xGQyi83XgdgYnih10P8f8RPczXrST8z8A+BhwR3cfDgGmvu77jvzs68jPm+4HVdVHGJwz8LsM9lJvBS4APt5t8h5+eqj/2wwi+Yc7fA/v64sMToy7DXg98Myq2lRVdwIvZfCL1O3Ac4HLd+Ln7sHgF6PbgO8y+IXhld26C4AVwNUMDtV/uVsm7ZLys08/SdKuIYM3rfm97vC3pG1wj1ySpIYZckmSGuahdUmSGuYeuSRJDTPkkiQ1rIl3dtt///1ryZIl4x5DkqSRWLly5W1VtWhHtm0i5EuWLGHFim29RFaSpMmS5MaZtxrw0LokSQ0z5JIkNcyQS5LUsCaeI5/OPffcw7p167j77mk/pXDiLFy4kMWLF7NgwYJxjyJJ2oU0G/J169axzz77sGTJEn72Uw4nT1WxadMm1q1bx9KlS8c9jiRpF9LsofW7776b/fbbb+IjDpCE/fbbb84cfZAk7bhmQw7MiYhvNZfuqyRpxzUdckmS5rpmnyOfarZ3WHf2s2Sqiqpit9383UiSNDpW5+ewdu1aDj/8cF784hdzzDHHMG/ePF72spfx2Mc+lic+8Yl86Utf4oQTTuDQQw/l8ssvB+Daa6/luOOOY9myZRx11FHccMMNAHzwgx/8yfKzzz6bLVu2jPOuSZIaYch/Ttdffz1nnHEGX/nKVwA44YQTWLlyJfvssw/nn38+V1xxBR/72Md49atfDcA73vEOzjnnHFatWsWKFStYvHgxq1ev5tJLL+Wqq65i1apVzJs3j0suuWScd0uS1IiJObQ+LocccgjHH388ALvvvjunnXYaAEceeSR77LEHCxYs4Mgjj2Tt2rUAPP7xj+f1r38969at4xnPeAaHHXYYV155JStXruTYY48F4K677uKAAw4Yy/2RJLXFkP+c9tprr59cXrBgwU/OLt9tt93YY489fnJ58+bNADz3uc/lcY97HJ/4xCc49dRTede73kVVceaZZ/KGN7xh9HdAktQ0D62P2Jo1azj00EN56UtfylOf+lSuvvpqTjrpJD7ykY+wYcMGAL73ve9x4407/ME3kqQ5zJCP2KWXXsqjH/1oli1bxnXXXccZZ5zBEUccwQUXXMApp5zCUUcdxcknn8z69evHPaokqQGpnX2d1RgsX768pn4e+erVqzn88MPHNNF4zMX7LElzUZKVVbV8R7b1OXJJc5ZvmNi2BvZDR8JD65IkNcyQS5LUsKZD3sLz+7NlLt1XSdKOazbkCxcuZNOmTXMicFs/j3zhwoXjHkWStItp9mS3xYsXs27dOjZu3DjuUUZi4cKFLF68eNxjSJJ2Mc2GfMGCBSxdunTcY0iSNFbNHlqXJEmGXJKkphlySZIaZsglSWqYIZckqWGGXJKkhhlySZIaZsglSWqYIZckqWGGXJKkhhlySZIaZsglSWqYIZckqWGGXJKkhhlySZIaZsglSWqYIZckqWGGXJKkhhlySZIaZsglSWqYIZckqWGGXJKkhhlySZIa1lvIkyxM8qUkX01ybZLXdsuXJvlikhuSXJpk975mkCRp0vW5R/4j4MSqegywDDgtyfHAm4ALq+ow4HbghT3OIEnSROst5DXwg+7qgu6rgBOBj3TLLwae3tcMkiRNul6fI08yL8kqYANwBfAt4I6q2txtsg44uM8ZJEmaZL2GvKq2VNUyYDFwHHD4dJtN971JzkqyIsmKjRs39jmmJEnNGslZ61V1B/AvwPHAg5LM71YtBm7ZxvdcVFXLq2r5okWLRjGmJEnN6fOs9UVJHtRd3hN4IrAa+AzwzG6zM4GP9zWDJEmTbv7Mm9xvBwEXJ5nH4BeGD1fVPyT5OvC3SS4AvgK8u8cZJEmaaL2FvKquBo6eZvkaBs+XS5Kkn5Pv7CZJUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDest5EkeluQzSVYnuTbJOd3yP01yc5JV3deT+5pBkqRJN7/Hn70Z+KOq+nKSfYCVSa7o1l1YVW/u8bYlSZoTegt5Va0H1neX70yyGji4r9uTJGkuGslz5EmWAEcDX+wWvSTJ1Unek+TBo5hBkqRJ1HvIk+wNfBQ4t6q+D7wdeASwjMEe+1u28X1nJVmRZMXGjRv7HlOSpCb1GvIkCxhE/JKqugygqm6tqi1VdS/wTuC46b63qi6qquVVtXzRokV9jilJUrP6PGs9wLuB1VX11qHlBw1t9lvANX3NIEnSpOvzrPUnAM8DvpZkVbfslcDpSZYBBawFzu5xBkmSJlqfZ61/Hsg0qz7Z121KkjTX+M5ukiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktQwQy5JUsMMuSRJDTPkkiQ1zJBLktSw3kKe5GFJPpNkdZJrk5zTLX9IkiuS3ND9+eC+ZpAkadL1uUe+GfijqjocOB74gyRHAC8Hrqyqw4Aru+uSJOl+6C3kVbW+qr7cXb4TWA0cDDwNuLjb7GLg6X3NIEnSpBvJc+RJlgBHA18EDqyq9TCIPXDANr7nrCQrkqzYuHHjKMaUJKk5vYc8yd7AR4Fzq+r7O/p9VXVRVS2vquWLFi3qb0BJkhrWa8iTLGAQ8Uuq6rJu8a1JDurWHwRs6HMGSZImWZ9nrQd4N7C6qt46tOpy4Mzu8pnAx/uaQZKkSTe/x5/9BOB5wNeSrOqWvRJ4I/DhJC8EvgM8q8cZJEmaaL2FvKo+D2Qbq0/q63YlSZpLfGc3SZIaZsglSWqYIZckqWGGXJKkhhlySZIaZsglSWqYIZckqWGGXJKkhhlySZIaZsglSWqYIZckqWHbDXmSE4cuL52y7hl9DSVJknbMTHvkbx66/NEp686f5VkkSdJOmink2cbl6a5LkqQRmynktY3L012XJEkjNtPnkR+a5HIGe99bL9NdX7rtb5MkSaMwU8ifNnT5zVPWTb0uSZJGbLshr6rPDl9PsgB4NHBzVW3oczBJkjSzmV5+9o4kj+ouPxD4KvB+4CtJTh/BfJIkaTtmOtntV6vq2u7yC4BvVNWRwGOBP+51MkmSNKOZQv7jocsnA38PUFXf7W0iSZK0w2YK+R1JnpLkaOAJwD8BJJkP7Nn3cJIkaftmOmv9bODPgV8Azh3aEz8J+ESfg0mSpJnNdNb6N4DTpln+KeBTfQ0lSZJ2zHZDnuTPt7e+ql46u+NIkqSdMdOh9d8HrgE+DNyC768uSdIuZaaQHwQ8C/htYDNwKfDRqrq978EkSdLMtnvWelVtqqp3VNVvAM8HHgRcm+R5oxhOkiRt30x75AAkOQY4ncFryf8RWNnnUJIkacfMdLLba4GnAKuBvwVeUVWbRzGYJEma2Ux75K8C1gCP6b7+LAkMTnqrqjqq3/EkSdL2zBRyP3NckqRd2ExvCHPjdMuTzAOeA0y7XpIkjcZMH2O6b5JXJPnLJKdk4A8ZHG5/9mhGlCRJ2zLTofUPALcD/wb8HvA/gd2Bp1XVqp5nkyRJM5gp5Id2nz9OkncBtwEPr6o7e59MkiTNaKaPMb1n64Wq2gJ824hLkrTrmGmP/DFJvt9dDrBnd33ry8/27XU6SZK0XTOdtT5vVINIkqSdN9OhdUmStAsz5JIkNcyQS5LUMEMuSVLDDLkkSQ0z5JIkNcyQS5LUsN5CnuQ9STYkuWZo2Z8muTnJqu7ryX3dviRJc0Gfe+TvA06bZvmFVbWs+/pkj7cvSdLE6y3kVfU54Ht9/XxJkjSe58hfkuTq7tD7g7e1UZKzkqxIsmLjxo2jnE+SpGaMOuRvBx4BLAPWA2/Z1oZVdVFVLa+q5YsWLRrVfJIkNWWkIa+qW6tqS1XdC7wTOG6Uty9J0qQZaciTHDR09beAa7a1rSRJmtlMn0d+vyX5EHACsH+SdcBrgBOSLAMKWAuc3dftS5I0F/QW8qo6fZrF7+7r9iRJmot8ZzdJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhrWW8iTvCfJhiTXDC17SJIrktzQ/fngvm5fkqS5oM898vcBp01Z9nLgyqo6DLiyuy5Jku6n3kJeVZ8Dvjdl8dOAi7vLFwNP7+v2JUmaC0b9HPmBVbUeoPvzgBHfviRJE2WXPdktyVlJViRZsXHjxnGPI0nSLmnUIb81yUEA3Z8btrVhVV1UVcuravmiRYtGNqAkSS0ZdcgvB87sLp8JfHzEty9J0kTp8+VnHwL+DXhkknVJXgi8ETg5yQ3Ayd11SZJ0P83v6wdX1enbWHVSX7cpSdJcs8ue7CZJkmZmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkho2f9wDCJJxT6D7q2rcE0ia69wjlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJapghlySpYYZckqSGGXJJkhpmyCVJatj8cdxokrXAncAWYHNVLR/HHJIktW4sIe/8RlXdNsbblySpeR5alySpYeMKeQGfTrIyyVnTbZDkrCQrkqzYuHHjiMeTJKkN4wr5E6rqGOBJwB8k+bWpG1TVRVW1vKqWL1q0aPQTSpLUgLGEvKpu6f7cAHwMOG4cc0iS1LqRhzzJXkn22XoZOAW4ZtRzSJI0CcZx1vqBwMeSbL39v6mqfxrDHJIkNW/kIa+qNcBjRn27kiRNIl9+JklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNM+SSJDXMkEuS1DBDLklSwwy5JEkNG0vIk5yW5Pok30zy8nHMIEnSJBh5yJPMA/4KeBJwBHB6kiNGPYckSZNgHHvkxwHfrKo1VfVj4G+Bp41hDkmSmjd/DLd5MHDT0PV1wOOmbpTkLOCs7uoPklw/gtnUj/2B28Y9RB+ScU8gbdfE/t2Dif/7d8iObjiOkE/30Nd9FlRdBFzU/zjqW5IVVbV83HNIc41/9+aGcRxaXwc8bOj6YuCWMcwhSVLzxhHy/wcclmRpkt2B5wCXj2EOSZKaN/JD61W1OclLgE8B84D3VNW1o55DI+VTJNJ4+HdvDkjVfZ6eliRJjfCd3SRJapghlySpYYZckqSGGXJJmkBJ9hr3DBoNQ65Zk+RFSQ7rLifJe5N8P8nVSY4Z93zSXJDkl5N8HVjdXX9Mkr8e81jqkSHXbDoHWNtdPh04ClgKnAf87zHNJM01FwKnApsAquqrwK+NdSL1ypBrNm2uqnu6y08B3l9Vm6rqnwEP80kjUlU3TVm0ZSyDaCQMuWbTvUkOSrIQOAn456F1e45pJmmuuSnJLwOVZPck/4PuMLsm0zg+NEWT69XACgbv2Hf51nfsS/LrwJpxDibNIb/P4Kmsgxl8tsWngT8Y60Tqle/splmVZD6wT1XdPrRsL2BZVV01vskkaTIZco1Eku9U1cPHPYc06ZIsAl4ELGHoqGtV/e64ZlK/PLSuUZnuc+glzb6PA//K4BwVT3KbAwy5RsVDP9JoPKCqXjbuITQ6hlyzJsn/ZfpgB9hvxONIc9U/JHlyVX1y3INoNHyOXLOmOzsdBi81Owy4F/gWcBdAVX12TKNJc0aSOxm8b8OPgHsY/CJdVbXvWAdTbwy5Zk2SBcDrgd8FvsPgH5DFwPuAVw69WYwkaZYYcs2aJBcCewPnVdWd3bJ9gTcDd1XVOeOcT5pkSX6pqq7b1ucaVNWXRz2TRsOQa9YkuQH4xZryP1WSecB1VXXYeCaTJl+Sd1bVi5J8ZprVVVUnjnwojYQh16xJ8o2q+sWdXSdJuv88a12z6etJzqiq9w8vTPJfgevGNJM0JyR5xvbWV9Vlo5pFo+UeuWZNkoOByxicpb6SwUvRjmVwFvtvVdXNYxxPmmhJ3rud1eU7u00uQ65Zl+RE4FEMzlq/tqquHPNIkjSxDLkkTZAkBwJ/Bjy0qp6U5Ajg8VX17jGPpp74eeSSNFneB3wKeGh3/RvAuWObRr0z5JI0Wfavqg8zeGdFqmozfnjKRDPkkjRZfphkP7rPPUhyPPDv4x1JffLlZ5I0Wc4DLgcekeQqYBHwzPGOpD55spskTZgk84FHMnjlyPV+zsFk89C6JE2QJM8C9qyqa4GnA5du6/3XNRkMuSRNlldV1Z1JfgU4FbgYePuYZ1KPDLkkTZatZ6j/JvD2qvo4sPsY51HPDLkkTZabk/wf4NnAJ5Psgf/WTzRPdpOkCZLkAcBpwNeq6oYkBwFHVtWnxzyaemLIJWkCJTkAWLj1elV9Z4zjqEcebpGkCZLkqUluAL4NfLb78x/HO5X6ZMglabK8Djge+EZVLQWeCFw13pHUJ0MuSZPlnqraBOyWZLeq+gywbNxDqT++RaskTZY7kuwNfA64JMkGYPOYZ1KPPNlNkiZIkr2Auxgccf0d4IHAJd1euiaQIZekCZVkf2BT+Q/9RPM5ckmaAEmOT/IvSS5LcnSSa4BrgFuTnDbu+dQf98glaQIkWQG8ksGh9IuAJ1XVF5L8EvChqjp6rAOqN+6RS9JkmF9Vn66qvwO+W1VfAKiq68Y8l3pmyCVpMtw7dPmuKes89DrBPLQuSRMgyRbgh0CAPYH/2LoKWFhVC8Y1m/plyCVJapiH1iVJapghlySpYYZckqSGGXJJkhpmyCVJatj/B3DH/LdmvxpQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c19614860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "figsize(8, 7)\n",
    "# Root mean squared error\n",
    "ax = yield_results.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'b')\n",
    "plt.title('Model RMSE Comparison'); plt.ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bayesian Liner Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will be using the GLM.from_formula function in PyMC3 which means we need to create a formula relating the explanatory variables (the features) to the response . The GLM.from_formula function call uses the R formula syntax which is shown below. When the model performs inference, it will draw samples from the posterior for each of the variables in the formula, along with an intercept and a variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y ~ cyield + irig_flag + days_under0 + dewPoint + precipAccumulation + precip + days_under_n10 + days_over42 + days_over32 + humidity + temp_delta + temperatureMin + apparentTemperatureMin + precipIntensity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_yield['y'] = list(y_train_yield)\n",
    "formula = 'y ~ ' + ' + '.join(['%s' % variable for variable in X_train_yield.columns[:-1]])\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [sd_log__, precipIntensity, apparentTemperatureMin, temperatureMin, temp_delta, humidity, days_over32, days_over42, days_under_n10, precip, precipAccumulation, dewPoint, days_under0, irig_flag, cyield, Intercept]\n",
      "100%|██████████| 2500/2500 [08:17<00:00,  5.02it/s]\n",
      "100%|██████████| 2500/2500 [10:16<00:00,  4.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "with pm.Model() as normal_model:\n",
    "    # Error term\n",
    "    # this is our model learning\n",
    "    pm.GLM.from_formula(formula, X_train_yield)\n",
    "    \n",
    "    # Perform Markov Chain Monte Carlo sampling\n",
    "    # Explain the idea of intelligent jumps and starting from the frequentist observation\n",
    "    normal_trace = pm.sample(draws=2000, tune = 500, njobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function GLM.from_formula parses the formula string, adds a random variable for each regressor and a likelihood for each random variable which by default is a normal distribution. The function then initializes the parameters (weights,  β\n",
    "β\n",
    " ) to sensible starting points using a frequentists estimate provided by statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_formula in module pymc3.glm.linear:\n",
      "\n",
      "from_formula(formula, data, priors=None, vars=None, family='normal', name='', model=None, offset=0.0) method of pymc3.model.InitContextMeta instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pm.GLM.from_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the priors and likelihood distributions above my specifying priors and family specifically.\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = {\"Intercept\": pm.Normal.dist(mu=0, sd=100),\n",
    "          \"Regressor\": pm.Laplace.dist(mu=0, b=np.sqrt(2))\n",
    "          }\n",
    "family=pm.glm.families.Binomial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(normal_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(normal_trace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
